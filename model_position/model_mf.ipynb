{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 라이브러리 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # pandas는 데이터를 다루기 위한 라이브러리\n",
    "from sklearn.model_selection import train_test_split  # train_test_split은 데이터를 train과 test로 나누기 위한 라이브러리\n",
    "from sklearn.preprocessing import StandardScaler  # StandardScaler는 데이터를 표준화하기 위한 라이브러리\n",
    "from sklearn.linear_model import LogisticRegression # LogisticRegression는 로지스틱 회귀를 위한 라이브러리\n",
    "from sklearn.ensemble import RandomForestClassifier # RandomForestClassifier는 랜덤 포레스트를 위한 라이브러리\n",
    "from xgboost import XGBClassifier # XGBClassifier는 XGBoost를 위한 라이브러리\n",
    "from sklearn.metrics import classification_report, accuracy_score # classification_report와 accuracy_score를 import\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. data load and Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1673, 64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mf_df = pd.read_csv(\"player_TopRate_position_JJINMAK/MF_combined.csv\")\n",
    "mf_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"isWin\", \"선수\",\"포지션_DF\", \"포지션_FW\", \"포지션_GK\", \"포지션_MF\", \"평점\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. VIF 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/5jun99/.pyenv/versions/3.8.19/lib/python3.8/site-packages/statsmodels/stats/outliers_influence.py:198: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  vif = 1. / (1. - r_squared_i)\n",
      "/Users/5jun99/.pyenv/versions/3.8.19/lib/python3.8/site-packages/statsmodels/regression/linear_model.py:1782: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return 1 - self.ssr/self.centered_tss\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Feature       VIF\n",
      "0           출전시간  2.595553\n",
      "1             득점  2.184842\n",
      "2             도움  1.239245\n",
      "3          오프사이드  1.085597\n",
      "4            프리킥  1.776839\n",
      "5            코너킥  2.029904\n",
      "6            스로인  1.599744\n",
      "7         드리블 성공  3.482886\n",
      "8     드리블 성공률(%)  3.322738\n",
      "9         Ishome  1.042553\n",
      "10      경합(지상)성공  2.438234\n",
      "11  경합(지상)성공률(%)  1.481216\n",
      "12      경합(공중)성공  2.042394\n",
      "13  경합(공중)성공률(%)  1.635209\n",
      "14          태클성공  2.589022\n",
      "15      태클성공률(%)  2.450978\n",
      "16          클리어링  1.575901\n",
      "17          인터셉트  1.540722\n",
      "18            차단  1.928038\n",
      "19            획득  1.827327\n",
      "20            블락  1.399481\n",
      "21           볼미스  1.096616\n",
      "22            파울  1.197122\n",
      "23           피파울  1.168166\n",
      "24            경고  1.109123\n",
      "25            퇴장  1.042681\n",
      "26      패스성공률(%)  6.621253\n",
      "27           키패스  1.966175\n",
      "28      공격진영패스성공  2.784978\n",
      "29  공격진영패스성공률(%)  1.476409\n",
      "30      수비진영패스성공  2.549316\n",
      "31  수비진영패스성공률(%)  1.398075\n",
      "32     롱패스성공률(%)  1.637549\n",
      "33   중거리패스성공률(%)  2.423263\n",
      "34     숏패스성공률(%)  3.197223\n",
      "35    전진패스성공률(%)  2.433153\n",
      "36     횡패스성공률(%)  1.911049\n",
      "37     백패스성공률(%)  1.331141\n",
      "38         크로스성공  3.797574\n",
      "39     크로스성공률(%)  2.221335\n",
      "40           탈압박  1.109798\n",
      "41            번호  1.083994\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "def calculate_vif(dataframe):\n",
    "    \"\"\"\n",
    "    주어진 데이터프레임의 VIF(분산 팽창 계수)를 계산.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): 독립 변수들로 이루어진 데이터프레임.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: 변수 이름과 해당 VIF 값.\n",
    "    \"\"\"\n",
    "    # 상수항 추가 (회귀식의 절편을 고려하기 위함)\n",
    "    X = add_constant(dataframe)\n",
    "    \n",
    "    # VIF 계산\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Feature\"] = X.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "    return vif_data\n",
    "\n",
    "mf_df_drop = mf_df.copy()\n",
    "mf_df_drop = mf_df.drop(columns= ['선수', \"포지션_DF\", \"포지션_FW\", \"포지션_GK\", \"포지션_MF\", \"평점\", \"isWin\"])\n",
    "# 예시 데이터\n",
    "# df: 독립 변수들로 이루어진 데이터프레임\n",
    "vif_result = calculate_vif(mf_df_drop)\n",
    "vif_result = vif_result[vif_result['VIF'] < 7].reset_index(drop=True)\n",
    "# VIF 결과 출력\n",
    "print(vif_result)\n",
    "feature_list = vif_result['Feature'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "# '성공'이 포함되었지만 '성공률'이 없는 항목을 제외한 리스트 만들기\n",
    "filtered_features = [\n",
    "    feature for feature in feature_list \n",
    "    if ('성공' in feature and '성공률' in feature) or '성공률' not in feature\n",
    "]\n",
    "\n",
    "print(len(filtered_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mf_df[filtered_features]\n",
    "y = mf_df[\"isWin\"]  # 타겟 변수\n",
    "\n",
    "# 학습/검증 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. LogisticRegression learning and evalutaion (RFE)\n",
    "- RFE(Recursive Feature Elimination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "선택된 피처들: Index(['평점', '득점', '도움', '슈팅', '블락된 슈팅', '오프사이드', '드리블 성공', '클리어링', '파울',\n",
      "       '키패스'],\n",
      "      dtype='object')\n",
      "모델 정확도: 0.9462\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "# 모델 선택\n",
    "model = LogisticRegression(max_iter=10000)\n",
    "\n",
    "# RFE를 통한 피처 선택\n",
    "selector = RFE(estimator=model, n_features_to_select=10)  # 선택할 피처 수 설정\n",
    "selector = selector.fit(X_train, y_train)\n",
    "\n",
    "# 선택된 피처 출력\n",
    "selected_features = X_train.columns[selector.support_]  # 선택된 피처들\n",
    "print(\"선택된 피처들:\", selected_features)\n",
    "\n",
    "# 선택된 피처를 사용한 모델 학습\n",
    "X_train_selected = selector.transform(X_train)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "# 모델 학습\n",
    "model.fit(X_train_selected, y_train)\n",
    "\n",
    "# 모델 성능 평가\n",
    "accuracy = model.score(X_test_selected, y_test)\n",
    "print(f\"모델 정확도: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. LogisticRegression learning and evalutaion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model = LogisticRegression(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model.fit(X_train_scaled, y_train)\n",
    "logistic_preds = logistic_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.86      0.89       169\n",
      "           1       0.87      0.93      0.90       166\n",
      "\n",
      "    accuracy                           0.90       335\n",
      "   macro avg       0.90      0.90      0.90       335\n",
      "weighted avg       0.90      0.90      0.90       335\n",
      "\n",
      "Accuracy: 0.8955223880597015\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression Performance:\")\n",
    "print(classification_report(y_test, logistic_preds))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, logistic_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. RandomForest and evalutaion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(random_state=42, n_estimators=100, max_depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model.fit(X_train, y_train)\n",
    "rf_preds = rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.91      0.94       169\n",
      "           1       0.91      0.99      0.95       166\n",
      "\n",
      "    accuracy                           0.95       335\n",
      "   macro avg       0.95      0.95      0.95       335\n",
      "weighted avg       0.95      0.95      0.95       335\n",
      "\n",
      "Accuracy: 0.9462686567164179\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nRandom Forest Performance:\")\n",
    "print(classification_report(y_test, rf_preds))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, rf_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. XGBoost and evalutaion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBClassifier(random_state=42, n_estimators=100, max_depth=10, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_preds = xgb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XGBoost Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.91      0.94       169\n",
      "           1       0.91      0.97      0.94       166\n",
      "\n",
      "    accuracy                           0.94       335\n",
      "   macro avg       0.94      0.94      0.94       335\n",
      "weighted avg       0.94      0.94      0.94       335\n",
      "\n",
      "Accuracy: 0.9373134328358209\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nXGBoost Performance:\")\n",
    "print(classification_report(y_test, xgb_preds))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, xgb_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Logistic, Random, XGBoost overfit check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Logistic Regression...\n",
      "Logistic Regression Performance:\n",
      "Train Accuracy: 0.7877, Test Accuracy: 0.8000\n",
      "Train Precision: 0.7722, Test Precision: 0.7797\n",
      "Train Recall: 0.8117, Test Recall: 0.8313\n",
      "Train F1 Score: 0.7915, Test F1 Score: 0.8047\n",
      "No Significant Overfitting Observed.\n",
      "\n",
      "Training Random Forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/5jun99/.pyenv/versions/3.8.19/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Performance:\n",
      "Train Accuracy: 0.9978, Test Accuracy: 0.9522\n",
      "Train Precision: 0.9955, Test Precision: 0.9261\n",
      "Train Recall: 1.0000, Test Recall: 0.9819\n",
      "Train F1 Score: 0.9977, Test F1 Score: 0.9532\n",
      "No Significant Overfitting Observed.\n",
      "\n",
      "Training XGBoost...\n",
      "XGBoost Performance:\n",
      "Train Accuracy: 1.0000, Test Accuracy: 0.9373\n",
      "Train Precision: 1.0000, Test Precision: 0.9096\n",
      "Train Recall: 1.0000, Test Recall: 0.9699\n",
      "Train F1 Score: 1.0000, Test F1 Score: 0.9388\n",
      "No Significant Overfitting Observed.\n"
     ]
    }
   ],
   "source": [
    "# 모델 초기화\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42, n_estimators=100, max_depth=10),\n",
    "    \"XGBoost\": XGBClassifier(random_state=42, n_estimators=100, max_depth=10, learning_rate=0.1),\n",
    "}\n",
    "\n",
    "# 성능 평가 함수 정의\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    accuracy = (tp + tn) / (tn + fp + fn + tp)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# 모델 학습 및 평가\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    \n",
    "    # 학습\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # 학습 데이터 성능\n",
    "    train_preds = model.predict(X_train)\n",
    "    train_accuracy, train_precision, train_recall, train_f1 = evaluate_model(y_train, train_preds)\n",
    "    \n",
    "    # 테스트 데이터 성능\n",
    "    test_preds = model.predict(X_test)\n",
    "    test_accuracy, test_precision, test_recall, test_f1 = evaluate_model(y_test, test_preds)\n",
    "    \n",
    "    # 성능 비교 출력\n",
    "    print(f\"{model_name} Performance:\")\n",
    "    print(f\"Train Accuracy: {train_accuracy:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Train Precision: {train_precision:.4f}, Test Precision: {test_precision:.4f}\")\n",
    "    print(f\"Train Recall: {train_recall:.4f}, Test Recall: {test_recall:.4f}\")\n",
    "    print(f\"Train F1 Score: {train_f1:.4f}, Test F1 Score: {test_f1:.4f}\")\n",
    "    \n",
    "    # 과적합 여부 확인\n",
    "    if train_accuracy - test_accuracy > 0.1 or train_f1 - test_f1 > 0.1:\n",
    "        print(\"Potential Overfitting Detected!\")\n",
    "    else:\n",
    "        print(\"No Significant Overfitting Observed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.8.19",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
